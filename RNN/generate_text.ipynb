{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "[47 73 25 53 63 38 32 23 72 26 26 26 69 25 53 53 44 23 77 25  0 48  2 48\n",
      " 38 54 23 25 32 38 23 25  2  2 23 25  2 48 75 38 16 23 38 45 38 32 44 23\n",
      " 35 40 73 25 53 53 44 23 77 25  0 48  2 44 23 48 54 23 35 40 73 25 53 53\n",
      " 44 23 48 40 23 48 63 54 23 13 30 40 26 30 25 44 37 26 26 74 45 38 32 44\n",
      " 63 73 48 40]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text에 등장하는 모든 문자들에 순차적으로 정수 인덱스를 부여해서 숫자로 표현된 인코딩된 텍스트를 생성 \n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "print(text[:100])\n",
    "print(encoded[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47 73 25 53 63 38 32 23 72 26 26 26 69 25 53 53 44 23 77 25  0 48  2 48\n",
      " 38 54 23 25 32 38 23 25  2  2 23 25  2 48 75 38 16 23 38 45 38 32 44 23\n",
      " 35 40]\n",
      "[73 25 53 63 38 32 23 72 26 26 26 69 25 53 53 44 23 77 25  0 48  2 48 38\n",
      " 54 23 25 32 38 23 25  2  2 23 25  2 48 75 38 16 23 38 45 38 32 44 23 35\n",
      " 40 73]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    # arr이 요소들에 대해 인덱스를 만들고, 열에 해당하는 인덱스에 1을 넣는다.\n",
    "    # arr.flatten()하여 1을 할당할 정수 열들을 지정한다\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    arr: 인코딩된 텍스트\n",
    "    n_seqs: 시퀀스 갯수\n",
    "    n_steps: 시퀀스 길이\n",
    "\n",
    "    n_seqs(갯수) x n_steps(길이) 크기의 배치를 생성하여 반환\n",
    "    \"\"\"\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr) // batch_size\n",
    "    \n",
    "    # 배치 사이즈만큼 딱 떨어지게 슬라이싱하여 가지고 온 뒤, 시퀀스 갯수 x row로 만들기\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # 인코딩된 텍스트에서 모든 배치에 대해 시퀀스 길이만큼 슬라이싱해서 온다\n",
    "        # 시퀀스 길이가 3이면 [1, 2, 3], [4, 5, 6] 이런식으로 모든 배치에 대해서 가져온다\n",
    "        x = arr[:, n:n + n_steps]\n",
    "        # 우리는 다음에 올 문자를 예측해야하므로 y에는 x에 담긴 시퀀스를 한칸씩 옮겨서 가져온다\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        # x를 1칸 시프트 한 값부터 끝까지 복사한 후, 마지막 값 n + n_steps값이 비므로 따로 넣어준다.\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
    "        # 맨 마지막에서는 값이 없을 수 있으므로 예외처리한다\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n",
    "\n",
    "n_seqs = 10\n",
    "n_steps = 50\n",
    "\n",
    "batches = get_batches(encoded, n_seqs, n_steps)\n",
    "x, y = next(batches)\n",
    "\n",
    "print(x[0, :n_steps])\n",
    "print(y[0, :n_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, hc=None, device=None, top_k=None):\n",
    "        ''' \n",
    "        char가 주어지면 다음에 올 char를 추론한다\n",
    "        '''\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.to(\"mps\")\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if hc is None:\n",
    "            hc = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            inputs = inputs.to(\"mps\")\n",
    "        \n",
    "        hc = tuple([each.data for each in hc])\n",
    "        out, hc = self.forward(inputs, hc)\n",
    "\n",
    "        # 확률 분포로 바꾼다\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if device is not None:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        # top_k 또는 top_ch에서 가장 확률이 높은 문자를 찾는다\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.cpu().numpy().squeeze()\n",
    "        \n",
    "        p = p.cpu().numpy().squeeze()\n",
    "\n",
    "        # 확률 값이 들어있는 p값을 정규화화 한 뒤 top_ch에서 p에 지정한 확률로 샘플링을 하여 하나를 고른다.\n",
    "        char = np.random.choice(top_ch, p=p / p.sum())\n",
    "            \n",
    "        return self.int2char[char], hc\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        RNN/LSTM은 기울기 소실/폭발 문제가 자주 일어나므로 초기화를 해준다.\n",
    "        \"\"\"\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # 가장 무난하게 쓰이는 균등분포로 초기화를 했으나 특별한 이유가 있진 않다.\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        \"\"\"\n",
    "        predict의 경우 LSTM의 초기 hidden state 값이 없으므로\n",
    "        n_layers x n_seqs x n_hidden 크기의 0으로 초기화한다.\n",
    "        \"\"\"\n",
    "        lstm_parmas = self.lstm.parameters()\n",
    "        h = next(lstm_parmas).data.new(self.n_layers, n_seqs, self.n_hidden).zero_()\n",
    "        c = next(lstm_parmas).data.new(self.n_layers, n_seqs, self.n_hidden).zero_()\n",
    "        return (h, c)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n",
      "Epoch: 1/25  Step: 100  Loss: 2.4442  Val Loss: 2.4632\n",
      "Epoch: 2/25  Step: 200  Loss: 2.1756  Val Loss: 2.2828\n",
      "Epoch: 3/25  Step: 300  Loss: 2.0262  Val Loss: 2.0947\n",
      "Epoch: 3/25  Step: 400  Loss: 1.8900  Val Loss: 2.0064\n",
      "Epoch: 4/25  Step: 500  Loss: 1.8512  Val Loss: 1.9264\n",
      "Epoch: 5/25  Step: 600  Loss: 1.7573  Val Loss: 1.8773\n",
      "Epoch: 6/25  Step: 700  Loss: 1.7211  Val Loss: 1.8337\n",
      "Epoch: 6/25  Step: 800  Loss: 1.6780  Val Loss: 1.8087\n",
      "Epoch: 7/25  Step: 900  Loss: 1.6326  Val Loss: 1.7716\n",
      "Epoch: 8/25  Step: 1000  Loss: 1.5865  Val Loss: 1.7397\n",
      "Epoch: 8/25  Step: 1100  Loss: 1.5615  Val Loss: 1.7304\n",
      "Epoch: 9/25  Step: 1200  Loss: 1.5335  Val Loss: 1.7057\n",
      "Epoch: 10/25  Step: 1300  Loss: 1.5219  Val Loss: 1.7008\n",
      "Epoch: 11/25  Step: 1400  Loss: 1.5514  Val Loss: 1.6819\n",
      "Epoch: 11/25  Step: 1500  Loss: 1.4686  Val Loss: 1.6750\n",
      "Epoch: 12/25  Step: 1600  Loss: 1.4774  Val Loss: 1.6658\n",
      "Epoch: 13/25  Step: 1700  Loss: 1.4632  Val Loss: 1.6548\n",
      "Epoch: 13/25  Step: 1800  Loss: 1.4637  Val Loss: 1.6400\n",
      "Epoch: 14/25  Step: 1900  Loss: 1.4339  Val Loss: 1.6412\n",
      "Epoch: 15/25  Step: 2000  Loss: 1.3985  Val Loss: 1.6305\n",
      "Epoch: 16/25  Step: 2100  Loss: 1.4168  Val Loss: 1.6352\n",
      "Epoch: 16/25  Step: 2200  Loss: 1.3902  Val Loss: 1.6165\n",
      "Epoch: 17/25  Step: 2300  Loss: 1.3564  Val Loss: 1.6163\n",
      "Epoch: 18/25  Step: 2400  Loss: 1.4131  Val Loss: 1.6223\n",
      "Epoch: 18/25  Step: 2500  Loss: 1.3674  Val Loss: 1.6078\n",
      "Epoch: 19/25  Step: 2600  Loss: 1.3669  Val Loss: 1.6106\n",
      "Epoch: 20/25  Step: 2700  Loss: 1.3732  Val Loss: 1.6163\n",
      "Epoch: 21/25  Step: 2800  Loss: 1.4020  Val Loss: 1.5925\n",
      "Epoch: 21/25  Step: 2900  Loss: 1.3175  Val Loss: 1.6103\n",
      "Epoch: 22/25  Step: 3000  Loss: 1.3134  Val Loss: 1.6125\n",
      "Epoch: 23/25  Step: 3100  Loss: 1.2933  Val Loss: 1.6243\n",
      "Epoch: 24/25  Step: 3200  Loss: 1.3251  Val Loss: 1.6048\n",
      "Epoch: 24/25  Step: 3300  Loss: 1.3231  Val Loss: 1.5872\n",
      "Epoch: 25/25  Step: 3400  Loss: 1.3204  Val Loss: 1.5891\n"
     ]
    }
   ],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, device=\"cpu\", print_every=10):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 데이터를 val_frac 비율로 나눈다.\n",
    "    val_idx = int(len(data) * (1 - val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if device is not None:\n",
    "        net.to(device)\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if device is not None:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs * n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # 과적합 방치를 위해 그래디언트 클리핑을 한다.\n",
    "            # L2 정규화 (벡터 길이 구하는 것)\n",
    "            # 보통 5 - 10 사이의 값이 좋다는 경험적 데이터에 근거한다.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if device is not None:\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(f\"Epoch: {e + 1}/{epochs} \",\n",
    "                      f\"Step: {counter} \",\n",
    "                      f\"Loss: {loss.item():.4f} \",\n",
    "                      f\"Val Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "\n",
    "net = Model(chars, n_hidden=512, n_layers=2)\n",
    "print(net)\n",
    "\n",
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, device=\"mps\", print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_1_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/v7mmst4s40q36mk9c9jhbhph0000gn/T/ipykernel_71196/3909229641.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna was streaming. Seryozha\n",
      "wanted on seeing his friends of the thing where the days there she\n",
      "could not have for a change for a minute, and the pord would\n",
      "so true, they went out into his chasinges were cringed to him them. \"You won't all shall at the moment, but\n",
      "however they showed and the still? His should be anyway,\"\n",
      "said Stepan Arkadyevitch, with a back of the charmed the princess of his wife.\n",
      "\n",
      "\"I can't here he have no sertimes to be a single way as the stop to her at hastes,\" he\n",
      "said, stepping and the doctor was as\n",
      "a considerable she filled his steps, and his shoots was he had been thought, how\n",
      "so if her heart and strong water, he said the country with him as he had to tone to herself to struck the simple hand of side of\n",
      "a seese she had bures, and his still said with those for mild of\n",
      "that her face, when she fluen to the partion and when she had\n",
      "always answered to the promistable are they had thinking of the door the table, the sheet and the people\n",
      "with his face what all all as she\n",
      "had asked and any one would be commisting his hat with something what was\n",
      "to be standing or his face, when there was no\n",
      "case in the sheet where he had seen, striking, and all and always that they were not strong to\n",
      "be stepping that then to all, that to his she had seen out in his cload who was in her. And he felt that\n",
      "it were anything and sad when he had been she saw that his wife's soul as he\n",
      "was settled that his words or a what he had been a parteraciate what were coming\n",
      "out; this was at a children, so he had an attention of a third that had stopped in\n",
      "the committees as it was a strained tanting of it. He went to the same, so who said with anything; and\n",
      "though she heard their state of her table or struggles, something as though were a seen. The\n",
      "head to the might for her and the peasants.\n",
      "\n",
      "\"I wouldn't suffer their staptings, and I'll be not strangely in since of a stand out\n",
      "that the master,\" he took him.\n",
      "\n",
      "\"Words what has taken a second over,\" said Stepan Arkadyevitch.\n",
      "\n",
      "How would not at \n"
     ]
    }
   ],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        net.to(\"mps\")\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for _ in range(size):\n",
    "        char, h = net.predict(chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "state_dict = torch.load(model_name)\n",
    "net = Model(state_dict['tokens'], n_hidden=state_dict['n_hidden'], n_layers=state_dict['n_layers'])\n",
    "net.load_state_dict(state_dict['state_dict'])\n",
    "print(sample(net, 2000, prime='Anna', top_k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단순 LSTM을 활용한 텍스트 생성 모델의 한계\n",
    "그럴싸하게 지어는 냈지만 말이 굉장히 어색한 것을 볼 수가 있는데 이는 작은 파라미터로 긴 문맥을 이해하지 않고 학습하기 때문이다. 단순히 다음에 올 확률이 높은 문자를 이어 붙이기만 하고\n",
    "LSTM의 파라미터(메모리)가 부족한 경우 장기적인 기억을 하는데 한계가 있기 떄문이다. 또한 글자 하나가 이어붙여질 때마다 오차는 누적되어 계속 초반 문장이 어색한데 어색한 것을 이어붙이는 식으로 \n",
    "만들어지기 때문에 매끄럽지 않을 수 있다. Top-K 샘플링으로 자연스러움을 조금 더 개선해줄 수 있으나 사실 이는 모델 자체의 한계가 크다.\n",
    "\n",
    "이 때문에 중요한 부분에 가중치를 부여하는 attention 기법을 쓰는 Transformer 모델이 등장하게 되지 않았나 싶다. Attention 자체가 사람이 정보를 찾는 방식을 흉내내는 것이라서 \n",
    "텍스트 생성과 비전 문제에서도 뛰어난 성능을 보여준 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
