{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech tagging with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'want': 1, 'chocolate': 2, 'latte': 3, 'he': 4, 'study': 5, 'the': 6, 'computer': 7, 'science': 8, 'she': 9, 'loves': 10, 'coding': 11, 'jun': 12, 'me': 13}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train = [\n",
    "    (\"I want chocolate latte\".lower().split(), [\"N\", \"V\", \"N\", \"N\"]),\n",
    "    (\"He study the computer science\".lower().split(), [\"N\", \"V\", \"D\", \"N\", \"N\"]),\n",
    "    (\"She loves coding\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"Jun loves me\".lower().split(), [\"N\", \"V\", \"N\"]), \n",
    "]\n",
    "\n",
    "test = [\n",
    "    (\"Jun study coding\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"Jun loves me\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"She loves latte\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"Computer science want chocolate study\".lower().split(), [\"N\", \"N\", \"V\", \"N\", \"V\"]),\n",
    "]\n",
    "\n",
    "word2idx = {}\n",
    "for sentence, tags in train:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "tag2idx = {\"D\": 0, \"N\": 1, \"V\": 2}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  1,  5,  7,  8])\n"
     ]
    }
   ],
   "source": [
    "def sequence_to_tensor(sentence, word2idx):\n",
    "    idxs = np.array([word2idx[word] for word in sentence])\n",
    "    return torch.from_numpy(idxs)\n",
    "\n",
    "# check out what prepare_sequence does for one of our training sentences:\n",
    "example = sequence_to_tensor(\"Jun want study computer science\".lower().split(), word2idx)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and define loss and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLLLoss is the negative log likelihood loss function\n",
    "텐서에 log 확률값이 들어있는 상황에서 사용하며, 이는 모델의 출력 단에 nn.LogSoftmax와 같은 함수를 적용하여 log 확률 형태로 변환한 뒤, 해당 log 확률과 실제 정답 레이블과 비교하여 손실을 계산하는 구조입니다.\n",
    "로그가능도가 클 수록(가능도가 높을 수록) 모델이 데이터를 잘 표현하고 있는 것이므로 이를 최소화 문제로 바꿔 optimizer를 적용하려면 음수를 취해 최소화 문제로 바꿔서 사용합니다.\n",
    "\n",
    "nn.CrossEntropyLoss 내부에서는 자동으로 nn.LogSoftmax를 적용하고 nn.NLLLoss를 계산하는 과정을 함께 처리합니다.\n",
    "즉, 별도로 log_softmax를 적용해야 하는지(= NLLLoss를 써야 하는지), 혹은 그냥 한 번에 CrossEntropyLoss를 쓸지는 모델의 출력 형태에 달려 있습니다.\n",
    "\n",
    "아래 모델에서는 `F.log_softmax(tag_outputs, dim=1)` 을 마지막 단에 적용하여 Log 확률 분포를 출력하므로 NLLLoss를 사용하여 음의 로그 가능도 기반으로 학습을 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0164, -1.2582, -1.0386],\n",
      "        [-1.0163, -1.2089, -1.0802],\n",
      "        [-1.0908, -1.2276, -0.9914],\n",
      "        [-1.1119, -1.2051, -0.9905],\n",
      "        [-1.0996, -1.2283, -0.9830]], grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([0, 0, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        ''' Define the feedforward behavior of the model.'''\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "model = Model(embedding_dim=5, hidden_dim=5, vocab_size=len(word2idx), tagset_size=len(tag2idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "test_sentence = \"The cheese loves the elephant\".lower().split()\n",
    "\n",
    "tag_scores = model(example)\n",
    "print(tag_scores)\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, loss: 0.00379\n",
      "Epoch: 40, loss: 0.00130\n",
      "Epoch: 60, loss: 0.00079\n",
      "Epoch: 80, loss: 0.00051\n",
      "Epoch: 100, loss: 0.00037\n",
      "Epoch: 120, loss: 0.00026\n",
      "Epoch: 140, loss: 0.00018\n",
      "Epoch: 160, loss: 0.00014\n",
      "Epoch: 180, loss: 0.00012\n",
      "Epoch: 200, loss: 0.00010\n",
      "Epoch: 220, loss: 0.00009\n",
      "Epoch: 240, loss: 0.00007\n",
      "Epoch: 260, loss: 0.00006\n",
      "Epoch: 280, loss: 0.00005\n",
      "Epoch: 300, loss: 0.00005\n",
      "tensor([[-1.7831e+01, -1.9073e-05, -1.0869e+01],\n",
      "        [-1.3935e+01, -1.1782e+01, -8.5830e-06],\n",
      "        [-1.3681e+01, -1.3113e-06, -1.6356e+01]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([1, 2, 1])\n",
      "['jun', 'study', 'coding'] -> ['N', 'V', 'N'] : ['N', 'V', 'N']\n",
      "['jun', 'loves', 'me'] -> ['N', 'V', 'N'] : ['N', 'V', 'N']\n",
      "['she', 'loves', 'latte'] -> ['N', 'V', 'N'] : ['V', 'V', 'N']\n",
      "['computer', 'science', 'want', 'chocolate', 'study'] -> ['N', 'N', 'V', 'N', 'V'] : ['N', 'N', 'V', 'N', 'V']\n"
     ]
    }
   ],
   "source": [
    "# normally these epochs take a lot longer \n",
    "# but with our toy data (only 3 sentences), we can do many epochs in a short time\n",
    "n_epochs = 300\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # get all sentences and corresponding tags in the training data\n",
    "    for sentence, tags in train:\n",
    "        \n",
    "        # zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # zero the hidden state of the LSTM, this detaches it from its history\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # prepare the inputs for processing by out network, \n",
    "        # turn all sentences and targets into Tensors of numerical indices\n",
    "        sentence_in = sequence_to_tensor(sentence, word2idx)\n",
    "        targets = sequence_to_tensor(tags, tag2idx)\n",
    "\n",
    "        # forward pass to get tag scores\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # compute the loss, and gradients \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the model parameters with optimizer.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print out avg loss per 20 epochs\n",
    "    if(epoch%20 == 19):\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch + 1, epoch_loss/len(train)))\n",
    "\n",
    "\n",
    "test_sentence = \"Jun loves coding\".lower().split()\n",
    "\n",
    "# see what the scores are after training\n",
    "inputs = sequence_to_tensor(test_sentence, word2idx)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)\n",
    "\n",
    "# print the most likely tag index, by grabbing the index with the maximum score!\n",
    "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)\n",
    "\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "for sentence, tags in test:\n",
    "    inputs = sequence_to_tensor(sentence, word2idx)\n",
    "    tag_scores = model(inputs)\n",
    "    _, predicted_tags = torch.max(tag_scores, 1)\n",
    "    print(f\"{sentence} -> {tags} : {[idx2tag[idx] for idx in predicted_tags.cpu().tolist()]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
