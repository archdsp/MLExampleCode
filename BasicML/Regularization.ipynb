{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [규제화(정칙화, Regularization)](https://en.wikipedia.org/wiki/Regularization_(mathematics))\n",
    "오버피팅을 막는 방법 중 하나\n",
    "## 정규화(Normalization)와 차이\n",
    "정규화는 데이터 전처리과정에서 feature 분포 범위를 맞춰주는 것이고 규제화는 모델에 제약을 거는 방법이다. dropout과 early stopping, weight decay등이 있다.  \n",
    "모델에 제약을 거는(페널티를 주는)것은 가중치 업데이트할 때 업데이트 값 뒤에 항(term)을 하나 더 더해주는 것이다. 왜 굳이 상쇄를 시키려 드냐면\n",
    "모델이 너무 오버피팅되서 weight값에 이상치가 생겼는데도 accuracy는 정확하게 나오는 경우를 막아주기 위해서다. loss가 줄어든다고 줄어드는 방향으로만 학습을 시키면  \n",
    "가중치는 loss가 줄어드는 방향으로 업데이트는 되지만 특정 가중치들이 크거나 작아지면서 해당 학습데이터셋에만 맞게 조율되버린다. \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/1200px-Regularization.svg.png)\n",
    "\n",
    "## $L_p Norm$\n",
    "$L_p$ 공간(르베그 공간)에서 norm을 $L_p norm$이라고 부르는데 이때 $L_p norm$을 수식으로 하면 다음과 같다. \n",
    "$$||x||_p = (\\sum^{n}_{i=1}{    |x_i|^p})^\\frac{1}{p}$$\n",
    "norm의 정의 자체가 벡터의 절대크기이므로 n차원 벡터의 차수 p에 따른 크기는 위의 수식처럼 구할 수 있게 된다.\n",
    "\n",
    "이때 차수 p에 따라 L1(Lasso), L2(Ridge)로 나뉘고 둘을 함께 쓴 네트워크의 경우 ElasticNet이라 부른다.\n",
    "\n",
    "**p값에 따른 norm 공간**\n",
    "p값이 1이하일 경우 global minima를 찾기 어려워진다고 한다.\n",
    "\n",
    "![](../docs_images/lp.png)\n",
    "\n",
    "### 어떻게 규제하는가\n",
    "아래 그림을 보면 가중치 값은 규제화에 의해 특정 값 범위를 벗어날 수 없다.  \n",
    "![](https://miro.medium.com/v2/resize:fit:1600/1*_e8BLNA749W_7yxi7hz-DA.gif)\n",
    "\n",
    "### p에 따른 규제화 term 범위\n",
    "\n",
    "\n",
    "### L1(Lasso)\n",
    "p=1 일 때 $L_p norm$으로 마름모 모양이 된다. loss 뒤에 절대값을 주었으니 이는 loss자체를 크게 만든다.  \n",
    "그래서 모델의 성능면에서는 가중치 업데이트가 한번에 많이 업데이트 되는 것을 방지한다고 볼 수 있다.\n",
    "\n",
    "\n",
    "![lasso_formula](../docs_images/lasso_formula.png)\n",
    "\n",
    "아래 그림을 보면 마름모 꼴로 나타나는 공간이기 때문에 w 해집합이 마름모의 축과 맞닿는 지점에 있을 가능성이 높다. \n",
    "왜냐면 결국 Error를 최소화해야 되는 부분이므로 제약항과 원 loss항이 거의 같아야되는 지점 즉 맞닿는 지점에 해집합이 있을 것이라는 뜻이 된다.  \n",
    "이때 어떤 축위에 있다는 것은 특정 $W_n$값이 경우에는 값이 0이라는 이야기이므로 L1 규제화가 더해진 회귀는 차원축소의 효과를 가지고 있음을 알 수 있다.\n",
    "\n",
    "![lasso](../docs_images/lasso.png)\n",
    "![](https://aunnnn.github.io/ml-tutorial/html/_images/img_l1_surface.png)\n",
    "![](https://aunnnn.github.io/ml-tutorial/html/_images/img_lasso_regression.png)\n",
    "![](https://aunnnn.github.io/ml-tutorial/html/_images/img_lasso_sol_30.png)\n",
    "![](https://e476rzxxeua.exactdn.com/wp-content/uploads/2020/08/L1-Regularization.png?strip=all&lossy=1&ssl=1)\n",
    "\n",
    "### L2(Ridge)\n",
    "L2는 차수 p가 2인 경우로 구 모양으로 페널티 공간이 생긴다고 보면 된다. 뒤에 더해지는 제약항이 제곱으로 더해지므로 w값에 따라 페널티 크기가 크게 달라진다.  \n",
    "제곱항을 쓰므로 L1보다 최적해로 가는 수렴이 빠르다. a = [1, 1, 1, 1, 1] b = [5, 0, ,0, 0, 0] 일 때 L1-norm은 크기가 5로 같지만 L2-norm크기는 1과 25로 차이가 많이 난다. 즉 결과에 영향을 미치는 값은 크게 결과에 영향을 주는 값은 작게 보내면서 수렴 속도가 빨라지는 것\n",
    "![](https://vitalflux.com/wp-content/uploads/2022/05/Ridge-regularization-linear-regression-model.png)\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQhCE6hyV94TWrmFTxCLur8rpdEWR10RCwpMBBqloXbBC9X2PCAukKX1CGqWj-Up7Q1fDE&usqp=CAU)\n",
    "\n",
    "### Reference\n",
    "* [Intuitive and Visual Explanation on the differences between L1 and L2 regularization](https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/)\n",
    "* [Regularization , Behind the scene-1](https://www.linkedin.com/pulse/regularization-episode-1-amr-mahmoud/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
